{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing supporting libraries\n",
    "# Current environment - Python 3\n",
    "\n",
    "#for algebra\n",
    "import numpy as np \n",
    "\n",
    "# for data(csv file) reading, processing and dataframe manipulation\n",
    "# also for using datetime functionality \n",
    "import pandas as pd\n",
    "\n",
    "# For decent presentation\n",
    "from termcolor import colored\n",
    " \n",
    "#below are libraries used for plotting and visualizing the graph\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# for label encoding\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and Exploring the given csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>Principal</th>\n",
       "      <th>terms</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>due_date</th>\n",
       "      <th>paid_off_time</th>\n",
       "      <th>past_due_days</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xqd20166231</td>\n",
       "      <td>PAIDOFF</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>09/08/16</td>\n",
       "      <td>10/07/16</td>\n",
       "      <td>9/14/2016 19:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>High School or Below</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xqd20168902</td>\n",
       "      <td>PAIDOFF</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>09/08/16</td>\n",
       "      <td>10/07/16</td>\n",
       "      <td>10/07/16 9:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Bechalor</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xqd20160003</td>\n",
       "      <td>PAIDOFF</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>09/08/16</td>\n",
       "      <td>10/07/16</td>\n",
       "      <td>9/25/2016 16:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>Bechalor</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xqd20160004</td>\n",
       "      <td>PAIDOFF</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>09/08/16</td>\n",
       "      <td>9/22/2016</td>\n",
       "      <td>9/22/2016 20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>college</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xqd20160005</td>\n",
       "      <td>PAIDOFF</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>09/09/16</td>\n",
       "      <td>10/08/16</td>\n",
       "      <td>9/23/2016 21:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>college</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Loan_ID loan_status  Principal  terms effective_date   due_date  \\\n",
       "0  xqd20166231     PAIDOFF       1000     30       09/08/16   10/07/16   \n",
       "1  xqd20168902     PAIDOFF       1000     30       09/08/16   10/07/16   \n",
       "2  xqd20160003     PAIDOFF       1000     30       09/08/16   10/07/16   \n",
       "3  xqd20160004     PAIDOFF       1000     15       09/08/16  9/22/2016   \n",
       "4  xqd20160005     PAIDOFF       1000     30       09/09/16   10/08/16   \n",
       "\n",
       "     paid_off_time  past_due_days  age             education  Gender  \n",
       "0  9/14/2016 19:31            NaN   45  High School or Below    male  \n",
       "1    10/07/16 9:00            NaN   50              Bechalor  female  \n",
       "2  9/25/2016 16:58            NaN   33              Bechalor  female  \n",
       "3  9/22/2016 20:00            NaN   27               college    male  \n",
       "4  9/23/2016 21:36            NaN   28               college  female  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untouched data\n",
    "orig_data = pd.read_csv('Loan payments data.csv') \n",
    "# all the alterations will be done in this\n",
    "data = orig_data.copy() \n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m\u001b[31mFEATURES OF THE LOAN DATASET:\u001b[0m\n",
      "\u001b[1m1\u001b[0m Loan_ID\n",
      "\u001b[1m2\u001b[0m loan_status\n",
      "\u001b[1m3\u001b[0m Principal\n",
      "\u001b[1m4\u001b[0m terms\n",
      "\u001b[1m5\u001b[0m effective_date\n",
      "\u001b[1m6\u001b[0m due_date\n",
      "\u001b[1m7\u001b[0m paid_off_time\n",
      "\u001b[1m8\u001b[0m past_due_days\n",
      "\u001b[1m9\u001b[0m age\n",
      "\u001b[1m10\u001b[0m education\n",
      "\u001b[1m11\u001b[0m Gender\n"
     ]
    }
   ],
   "source": [
    "# printing the features in a list format for better understanding\n",
    "print(colored(\"FEATURES OF THE LOAN DATASET:\",'red',attrs=['bold','underline']))\n",
    "for i in range(len(data.columns)):\n",
    "               print(colored(i+1, attrs=['bold']), data.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 454 entries, 0 to 453\n",
      "Data columns (total 11 columns):\n",
      "Loan_ID           454 non-null object\n",
      "loan_status       454 non-null object\n",
      "Principal         454 non-null int64\n",
      "terms             454 non-null int64\n",
      "effective_date    454 non-null object\n",
      "due_date          454 non-null object\n",
      "paid_off_time     369 non-null object\n",
      "past_due_days     170 non-null float64\n",
      "age               454 non-null int64\n",
      "education         454 non-null object\n",
      "Gender            454 non-null object\n",
      "dtypes: float64(1), int64(3), object(7)\n",
      "memory usage: 39.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take Away:\n",
    "From the above information it is clear that out of 454 total enteries, 2 features including paid_off_time and past_due_days have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mFEATURE_NAME,  TOTAL_NULL_VALUES  and  MISSING_VALUE_PERCENTAGE\n",
      "\u001b[0m\n",
      "paid_off_time [85, 18.72]\n",
      "past_due_days [284, 62.56]\n"
     ]
    }
   ],
   "source": [
    "## Finding null values if exists in any column of the dataset\n",
    "print(colored(\"FEATURE_NAME,  TOTAL_NULL_VALUES  and  MISSING_VALUE_PERCENTAGE\\n\", 'blue',attrs=['bold']))  \n",
    "\n",
    "for i in range(len(data.columns)):\n",
    "    current_feature = data[ data.columns[i] ]    \n",
    "    if  (current_feature.isnull().values.any()):        \n",
    "        sum_of_null_values = current_feature.isnull().sum()      \n",
    "        null_value_percentage = (sum_of_null_values/len(current_feature))*100      \n",
    "        \n",
    "        result = ([sum_of_null_values,  float(\"{0:.2f}\".format(null_value_percentage))])\n",
    "        print(data.columns[i], result)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Since, the percentage of missing values is quite considerable especially for past_due_days feature, we should test and try how its removal might affect the accuracy of the intended prediction.\n",
    "\n",
    "Making a test_try dataframe of the same dataset for understading how necessary these columns are will be discussed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NOT USED\n",
    "## make all values None for the null values only in data \n",
    "# data = data.fillna( 'NAN' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m \u001b[1m\u001b[31mloan_status\u001b[0m ['PAIDOFF' 'COLLECTION' 'COLLECTION_PAIDOFF'] \n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m \u001b[1m\u001b[31mPrincipal\u001b[0m [1000  300  800  900  700  500] \n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m \u001b[1m\u001b[31mterms\u001b[0m [30 15  7] \n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m \u001b[1m\u001b[31meffective_date\u001b[0m ['09/08/16' '09/09/16' '09/10/16' '09/11/16' '09/12/16' '9/13/2016'\n",
      " '9/14/2016'] \n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m \u001b[1m\u001b[31mdue_date\u001b[0m ['10/07/16' '9/22/2016' '10/08/16' '9/15/2016' '9/24/2016' '9/16/2016'\n",
      " '10/09/16' '10/10/16' '9/25/2016' '9/17/2016' '11/09/16' '10/25/2016'\n",
      " '9/18/2016' '9/26/2016' '10/26/2016' '10/11/16' '11/10/16' '9/19/2016'\n",
      " '10/12/16' '9/27/2016' '9/28/2016' '10/13/2016' '11/12/16' '9/23/2016'\n",
      " '11/07/16'] \n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m \u001b[1m\u001b[31mpaid_off_time\u001b[0m ['9/14/2016 19:31' '10/07/16 9:00' '9/25/2016 16:58' '9/22/2016 20:00'\n",
      " '9/23/2016 21:36' '09/09/16 13:45' '10/07/16 23:07' '10/05/16 20:33'\n",
      " '10/08/16 16:00' '9/24/2016 13:00' '09/11/16 19:11' '10/09/16 16:00'\n",
      " '10/07/16 23:32' '9/13/2016 21:57' '9/15/2016 14:27' '9/24/2016 16:00'\n",
      " '9/27/2016 14:21' '9/23/2016 18:49' '10/05/16 22:05' '9/23/2016 7:42'\n",
      " '10/09/16 9:00' '10/08/16 17:09' '10/09/16 23:00' '10/03/16 12:50'\n",
      " '9/29/2016 12:18' '9/21/2016 20:16' '9/23/2016 8:21' '9/22/2016 19:17'\n",
      " '10/09/16 17:33' '9/24/2016 14:41' '10/07/16 21:48' '10/09/16 17:44'\n",
      " '10/09/16 7:24' '9/25/2016 21:49' '9/25/2016 9:00' '10/10/16 16:00'\n",
      " '9/25/2016 13:00' '10/10/16 11:33' '9/25/2016 14:36' '9/24/2016 9:00'\n",
      " '9/20/2016 15:00' '9/21/2016 22:29' '09/12/16 22:17' '10/08/16 14:14'\n",
      " '10/09/16 8:53' '10/10/16 9:00' '9/25/2016 19:21' '9/13/2016 4:34'\n",
      " '9/25/2016 16:00' '10/07/16 2:33' '9/24/2016 11:40' '9/22/2016 6:38'\n",
      " '9/30/2016 21:12' '9/24/2016 13:42' '10/08/16 7:25' '09/12/16 11:40'\n",
      " '9/25/2016 14:38' '9/25/2016 23:00' '10/08/16 12:04' '9/14/2016 22:05'\n",
      " '9/24/2016 13:27' '9/22/2016 21:18' '9/24/2016 22:53' '10/08/16 13:12'\n",
      " '10/09/16 13:49' '9/30/2016 14:29' '9/21/2016 16:18' '9/13/2016 14:53'\n",
      " '10/10/16 13:00' '11/09/16 9:00' '9/17/2016 13:01' '9/21/2016 9:35'\n",
      " '9/24/2016 20:33' '9/17/2016 9:00' '9/24/2016 0:12' '9/21/2016 12:43'\n",
      " '10/08/16 20:49' '9/20/2016 5:38' '10/10/16 9:01' '10/08/16 16:55'\n",
      " '9/25/2016 23:48' '9/22/2016 13:15' '9/23/2016 13:31' '10/08/16 17:19'\n",
      " '10/09/16 13:05' '9/20/2016 20:47' '10/10/16 13:01' '10/08/16 21:39'\n",
      " '10/07/16 14:23' '10/06/16 15:25' '10/08/16 6:56' '9/16/2016 11:58'\n",
      " '10/10/16 16:01' '9/27/2016 7:02' '10/25/2016 9:00' '9/24/2016 11:02'\n",
      " '9/29/2016 18:57' '9/25/2016 12:24' '9/26/2016 4:41' '9/22/2016 15:44'\n",
      " '10/10/16 16:13' '9/30/2016 7:12' '9/23/2016 11:26' '09/12/16 10:26'\n",
      " '9/22/2016 21:45' '10/09/16 20:28' '10/01/16 16:48' '9/25/2016 9:01'\n",
      " '9/15/2016 20:36' '9/21/2016 15:33' '9/29/2016 13:36' '9/22/2016 20:51'\n",
      " '11/09/16 23:00' '9/18/2016 9:00' '9/26/2016 9:00' '9/24/2016 10:14'\n",
      " '10/26/2016 9:00' '9/23/2016 20:30' '9/13/2016 20:17' '10/10/16 7:01'\n",
      " '10/11/16 13:00' '9/24/2016 14:55' '9/25/2016 20:56' '9/22/2016 10:49'\n",
      " '9/25/2016 22:09' '10/11/16 9:00' '9/26/2016 19:33' '11/10/16 16:00'\n",
      " '10/11/16 16:00' '9/26/2016 13:00' '9/26/2016 16:00' '9/17/2016 7:39'\n",
      " '9/22/2016 10:30' '10/10/16 20:41' '10/10/16 8:04' '9/24/2016 11:00'\n",
      " '9/17/2016 9:25' '10/07/16 11:53' '9/25/2016 8:39' '10/10/16 20:28'\n",
      " '10/01/16 10:18' '9/19/2016 21:07' '9/30/2016 14:38' '9/14/2016 20:31'\n",
      " '9/24/2016 16:15' '10/10/16 15:49' '11/10/16 13:00' '9/23/2016 10:32'\n",
      " '9/30/2016 14:03' '10/09/16 14:17' '9/20/2016 8:26' '10/11/16 23:00'\n",
      " '9/24/2016 20:47' '10/11/16 9:01' '9/16/2016 14:52' '10/09/16 10:00'\n",
      " '09/12/16 14:40' '9/23/2016 21:58' '10/08/16 18:48' '10/10/16 16:41'\n",
      " '10/11/16 13:01' '9/16/2016 2:34' '9/21/2016 8:11' '9/26/2016 23:00'\n",
      " '9/23/2016 14:01' '9/25/2016 13:29' '9/25/2016 14:50' '10/08/16 15:35'\n",
      " '10/11/16 16:01' '9/26/2016 19:35' '10/09/16 21:28' '10/07/16 16:45'\n",
      " '9/24/2016 12:13' '9/14/2016 23:02' '10/08/16 11:03' '9/25/2016 19:31'\n",
      " '9/14/2016 19:48' '10/12/16 23:00' '9/25/2016 12:48' '9/26/2016 21:18'\n",
      " '10/07/16 10:22' '9/26/2016 6:17' '9/22/2016 16:57' '10/09/16 21:57'\n",
      " '10/04/16 12:59' '9/17/2016 20:51' '10/08/16 15:51' '9/27/2016 9:00'\n",
      " '9/16/2016 15:57' '10/12/16 9:00' '9/26/2016 7:48' '9/21/2016 16:53'\n",
      " '10/11/16 0:29' '9/25/2016 10:37' '9/27/2016 13:00' '9/26/2016 15:10'\n",
      " '9/24/2016 12:46' '9/28/2016 9:00' '10/13/2016 9:00' '10/06/16 12:09'\n",
      " '10/14/2016 11:03' '10/08/16 17:12' '11/12/16 9:00' '10/13/2016 13:00'\n",
      " '9/27/2016 15:52' '9/28/2016 13:00' '9/15/2016 0:43' '10/10/16 10:25'\n",
      " '9/27/2016 20:41' '10/06/16 6:51' '10/12/16 6:25' nan '10/10/16 11:45'\n",
      " '9/27/2016 17:00' '11/20/2016 14:10' '9/28/2016 15:38' '9/26/2016 17:22'\n",
      " '10/21/2016 14:00' '9/26/2016 11:03' '11/05/16 15:39' '11/22/2016 15:53'\n",
      " '9/29/2016 10:30' '10/10/16 15:18' '11/05/16 10:49' '9/27/2016 17:10'\n",
      " '9/26/2016 11:35' '10/12/16 9:59' '9/27/2016 17:14' '10/11/16 12:45'\n",
      " '9/28/2016 11:38' '10/07/16 13:21' '11/04/16 15:37' '9/28/2016 17:39'\n",
      " '10/12/16 9:52' '9/29/2016 15:12' '10/12/16 11:17' '11/10/16 22:58'\n",
      " '11/03/16 15:23' '10/11/16 16:44' '10/11/16 11:02' '10/12/16 13:17'\n",
      " '10/11/16 17:22' '9/28/2016 14:02' '9/29/2016 13:42' '9/19/2016 15:00'\n",
      " '10/12/16 14:32' '10/11/16 11:33' '10/11/16 16:27' '11/15/2016 15:27'\n",
      " '10/11/16 16:13' '10/17/2016 10:06' '11/14/2016 13:15' '10/24/2016 16:20'\n",
      " '9/27/2016 16:35' '10/11/16 11:48' '11/07/16 19:21' '10/12/16 16:22'\n",
      " '9/27/2016 17:24' '11/04/16 11:07' '11/02/16 9:39' '10/13/2016 18:18'\n",
      " '10/11/16 11:29' '10/13/2016 16:27' '9/29/2016 11:19' '9/28/2016 11:17'\n",
      " '10/14/2016 11:04' '10/17/2016 17:40' '9/28/2016 9:42' '11/18/2016 15:52'\n",
      " '10/30/2016 14:19' '10/13/2016 15:10' '9/28/2016 13:36' '9/28/2016 15:34'\n",
      " '11/17/2016 11:55' '11/15/2016 18:51' '9/30/2016 10:23' '11/11/16 17:17'\n",
      " '10/12/16 12:54' '10/15/2016 9:48' '10/27/2016 11:14' '10/15/2016 14:14'\n",
      " '12/02/16 9:45' '9/28/2016 15:02' '11/04/16 14:46' '11/16/2016 12:12'\n",
      " '10/14/2016 19:02' '9/28/2016 11:34' '11/09/16 18:12' '10/31/2016 13:07'\n",
      " '10/20/2016 17:38' '11/07/16 8:55' '10/12/16 18:26' '10/25/2016 13:44'\n",
      " '9/29/2016 15:07' '9/27/2016 11:40' '10/18/2016 19:08' '10/15/2016 9:23'] \n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m \u001b[1m\u001b[31mpast_due_days\u001b[0m [ nan  76.  61.  75.  60.  59.  74.  29.  44.   2.   4.  13.   5.   3.  12.\n",
      "  27.   1.  25.  24.  36.   7.  14.  28.  23.   6.  38.  19.  52.  51.   9.] \n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m \u001b[1m\u001b[31mage\u001b[0m [45 50 33 27 28 35 29 36 26 39 40 32 43 25 34 31 37 24 21 30 22 44 46 38 23\n",
      " 20 47 42 19 51 41 18 49] \n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m \u001b[1m\u001b[31meducation\u001b[0m ['High School or Below' 'Bechalor' 'college' 'Master or Above'] \n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m \u001b[1m\u001b[31mGender\u001b[0m ['male' 'female'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LOOKING FOR UNIQUE VALUES OF COLUMNS OF INTEREST\n",
    "## coi = [1,2,3,4,5,6,,9,10]\n",
    "\n",
    "# Unique values for each attribute/feature\n",
    "for i in range(1,11):\n",
    "    print(colored(i, \"red\", attrs=['bold']),\n",
    "          colored(data.columns[i], \"red\", attrs=['bold'] ), \n",
    "          data[data.columns[i]].unique(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take Away:\n",
    "* 'paid_off_time' attribute is a date-time stamp. Though it has 18.72% of missing values, extracting just the date and onserving how it affects the prediction of 'loan_status' will be a wise thing to do.\n",
    "\n",
    "* For this a new column is attched to the copy of original dataframe by the name 'paid_off_date'. With the introduction of this column 'paid_off_time' can be dropped off from this copy dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pakhi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES ARE: 85\n",
      "Hence rechecked!\n"
     ]
    }
   ],
   "source": [
    "# PAID OFF TIME is a date-time stamp \n",
    "# Label encoder won't work on it and give it a unique integer\n",
    "# Since time is of no such important but date is, \n",
    "# it is wise to replace this column with a new paid_off_date column\n",
    "# which will only have date\n",
    "\n",
    "## Method 1: the method below gives warning but the dates produced seemed to be the same as done by method 2\n",
    "data['paid_off_date'] = None\n",
    "count = 0\n",
    "\n",
    "for i in range(len(data.paid_off_time)):\n",
    "    \n",
    "     if not pd.isnull(data['paid_off_time'][i]):\n",
    "        count = count + 1\n",
    "        \n",
    "        data['paid_off_date'][i] = pd.to_datetime(data['paid_off_time'][i]).date().strftime('%m/%d/%Y')\n",
    "\n",
    "print('MISSING VALUES ARE:', 454-count)\n",
    "print(\"Hence rechecked!\")\n",
    "\n",
    "### Method 2\n",
    "# data['paid_off_date'] = pd.DatetimeIndex(data.paid_off_time).normalize()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the unique values of the new column\n",
    "\n",
    "print(colored(\"THE UNIQUE VALUED OF 'paid_off_date'\\n\", \"red\", attrs=['bold']))\n",
    "print(data['paid_off_date'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rechecking the null or missing values\n",
    "\n",
    "# count = 0\n",
    "# for i in range(len(data.paid_off_date)):\n",
    "    \n",
    "#     if data['paid_off_date'][i] == 'nan':\n",
    "#         count = count + 1\n",
    "# print(\"MISSING VALUE COUNT:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With the introduction of paid_off_date, paid_off_time can be deleted\n",
    "\n",
    "data = data.drop(['paid_off_time'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"THE UNIQUE VALUED OF 'past_due_days'\\n\", \"red\", attrs=['bold']))\n",
    "print(data['paid_off_date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VISUALIZING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before being able to visualize each and every feature it is must that all \n",
    "# strings are in integer form\n",
    "# and null values are treated as a category\n",
    "# For this we make use of label encoder\n",
    "\n",
    "le=preprocessing.LabelEncoder()\n",
    "\n",
    "#String values\n",
    "data['loan_status'] = le.fit_transform(data['loan_status'])\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "data['education'] = le.fit_transform(data['education'])\n",
    "data['effective_date'] = le.fit_transform(data['effective_date'])\n",
    "data['due_date'] = le.fit_transform(data['due_date'])\n",
    "data['Loan_ID'] = le.fit_transform(data['Loan_ID'])\n",
    "\n",
    "\n",
    "#nan values - treated as a category \n",
    "data['past_due_days'] = le.fit_transform(data['past_due_days'])\n",
    "\n",
    "##won't work on date-time format (no need too)\n",
    "# data['paid_off_date'] = le.fit_transform(data['paid_off_date'])                       \n",
    "                                              \n",
    "\n",
    "## Can't apply the same for Nan valued paid_off_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now seeing the changed dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEATMAP VISUALIZARION (does multivarite data analysis)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "corr = data.corr()\n",
    "heatmap = sns.heatmap(corr, annot=True, linewidths=.5,  ax=ax)\n",
    "\n",
    "loc, labels = plt.xticks()\n",
    "heatmap.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "\n",
    "print(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER FUNCTION TO GET HEIGT OF EACH BAR IN BAR PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_height(ax):\n",
    "    for box in ax.patches:\n",
    "        ax.annotate(box.get_height(), (box.get_x(), box.get_height()*1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAPHICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "ax = sns.countplot(data['loan_status'])\n",
    "ax.set_title(\"Analysing of Loan Status\")\n",
    "get_height(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data.groupby('loan_status')['Loan_ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal amount and loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "ax = sns.countplot(x='Principal', hue = \"loan_status\", data=data)\n",
    "plt.legend(loc='upper left', fontsize='12')\n",
    "\n",
    "get_height(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data.groupby(['Principal','loan_status'])['Loan_ID'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.loan_status, data.Principal, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "ax = sns.countplot(hue='terms', x = \"loan_status\", data=data)\n",
    "plt.legend(loc='upper left', fontsize='12')\n",
    "\n",
    "get_height(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = pd.DataFrame(data.groupby('terms')['Loan_ID'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "pos = np.arange(len(data.terms.unique()))\n",
    "# Terms can be labelled as weekly (7), biweekly (14), monthly (30)\n",
    "mapTerms={7:'weekly', 15:'bi-weekly', 30: 'monthly'}\n",
    "pie = ax.pie(terms.Loan_ID.values, labels=[mapTerms[l] for l in list(terms.index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  EFFECTIVE DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "ax = sns.countplot(x='effective_date', hue='loan_status', data= data)\n",
    "plt.legend(loc='upper left', fontsize='12')\n",
    "\n",
    "get_height(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Gender')['Gender'].agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.countplot( x = \"loan_status\",hue = \"Gender\", data=data)\n",
    "plt.legend(loc='upper right', fontsize='12')\n",
    "get_height(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.loan_status, data.Principal, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data.drop(['loan_status'], axis = 1)\n",
    "y = data.loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdata.drop('loan_status', axis=1, inplace=True)\n",
    "# label = cdata.pop('loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING LOAN_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, label_train, label_test = train_test_split(x, y, test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logis = LogisticRegression()\n",
    "logis.fit(data_train, label_train)\n",
    "logis_score_train = logis.score(data_train, label_train)\n",
    "print(\"Training score: \",logis_score_train)\n",
    "logis_score_test = logis.score(data_test, label_test)\n",
    "print(\"Testing score: \",logis_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "dt = RandomForestClassifier()\n",
    "dt.fit(data_train, label_train)\n",
    "dt_score_train = dt.score(data_train, label_train)\n",
    "print(\"Training score: \",dt_score_train)\n",
    "dt_score_test = dt.score(data_test, label_test)\n",
    "print(\"Testing score: \",dt_score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "### Create function to evaluate the score of each classification model\n",
    "def eval_model_classifier(model, data, target, split_ratio):\n",
    "    trainX, testX, trainY, testY = train_test_split(data, target, train_size=split_ratio, random_state=0)\n",
    "    model.fit(trainX, trainY)    \n",
    "    return model.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_estimators_array = np.array([1,5,10,50,100,200,500]) \n",
    "num_smpl = 5 # Test run the model according to samples_number\n",
    "num_grid = len(num_estimators_array)\n",
    "score_array_mu = np.zeros(num_grid) # Keep mean\n",
    "score_array_sigma = np.zeros(num_grid) # Keep Standard deviation \n",
    "j=0\n",
    "\n",
    "# print(\"{}: RandomForestClassification Starts!\".format(now()))\n",
    "for n_estimators in num_estimators_array:\n",
    "    score_array = np.zeros(num_smpl) # Initialize\n",
    "    for i in range(0,num_smpl):\n",
    "        rf_class = RandomForestClassifier(n_estimators = n_estimators, n_jobs=1, criterion=\"gini\")\n",
    "        score_array[i] = eval_model_classifier(rf_class, data, label, 0.8)\n",
    "#         print(\"{}: Try {} with n_estimators = {} and score = {}\".format(now(), i, n_estimators, score_array[i]))\n",
    "    score_array_mu[j], score_array_sigma[j] = np.mean(score_array), np.std(score_array)\n",
    "    j=j+1\n",
    "\n",
    "# print(\"{}: RandomForestClassification Done!\".format(now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,3))\n",
    "plt.errorbar(num_estimators_array, score_array_mu, yerr=score_array_sigma, fmt='k.-')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"number of estimators\",size = 16)\n",
    "plt.ylabel(\"accuracy\",size = 16)\n",
    "plt.xlim(0.9,600)\n",
    "plt.ylim(0.3,0.8)\n",
    "plt.title(\"Random Forest Classifier\", size = 18)\n",
    "plt.grid(which=\"both\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
